# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ziQ53DYyQdRod8Z0Zy03Jw4_qNTBkjCN
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

dataset_1_path='/content/drive/MyDrive/Colab Notebooks/sentiment_analysis/dataset_1.csv'
dataset_2_path='/content/drive/MyDrive/Colab Notebooks/sentiment_analysis/dataset_2.csv'

df_1=pd.read_csv(dataset_1_path)

df_version1=df_1.filter(['asins','reviews.text','reviews.rating', 'reviews.numHelpful'])

df_version1.count()

df_2=pd.read_csv(dataset_2_path)

df_version2=df_2.filter(['asins','reviews.text','reviews.rating','review.numHelpful'])

df_version2.count()

df_version1['asins'].value_counts()

df_version2['asins'].value_counts()

df=df_version2.append(df_version1)

df.describe()

df.nunique(axis=1)

## final datraset is 'df'

## data preprocessing in sentiment analysis

##lets finds the null or NaN value in each column
import nltk
import spacy
import string
import re

## convert to string
df['reviews.text']=df['reviews.text'].astype(str)

#convert to lower case
df['review_lower']=df['reviews.text'].str.lower()

## remove  punctuation and stopwords
string.punctuation

PUNC_TO_REMOVE=string.punctuation

def remove_punctuation(text):
  return text.translate(str.maketrans('','',PUNC_TO_REMOVE))

df['reviews.numHelpful']=df['reviews.numHelpful'].fillna(0)

df.drop(['asins'],axis=1,inplace=True)

## stopword using nltk
!pip3 install nltk
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')

STOPWORDS=set(stopwords.words('english'))

def remove_stopword_and_punctuation(review):
  return ' '.join([word for word in review.split() if word not in STOPWORDS and word not in string.punctuation ])

df['reviews_wo_punc_and_stopwords']=df['review_lower'].apply(lambda review: remove_stopword_and_punctuation(review))

df['reviews_wo_punc_and_stopwords']=df['reviews_wo_punc_and_stopwords'].apply(lambda text: remove_punctuation(text))

df.head()

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

## stemming and Lemmatization
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
lemmatizer=WordNetLemmatizer()

wordnet_map={
    'N':wordnet.NOUN,
    'V':wordnet.VERB,
    'A':wordnet.ADJ,
    'R':wordnet.ADV
}

def lemmatize_words(review):
  pos_tagged_text=nltk.pos_tag(review.split())
  return ' '.join([lemmatizer.lemmatize(word,wordnet_map.get(pos[0],wordnet.NOUN)) for word, pos in pos_tagged_text])

df['lemmatize_text']=df['reviews_wo_punc_and_stopwords'].apply(lambda review : lemmatize_words(review))

df.head()

## model defination
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer,one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D,Flatten
from sklearn.model_selection import  train_test_split
from keras.utils.np_utils import to_categorical

## tokenization
def review_tokenization(review):
  return word_tokenize(review)

df['word_review_token']=df['lemmatize_text'].apply(lambda review: review_tokenization(review) )

## lets remove number from reviews

import re
def remove_number(review):
  return ' '.join([ s for s in review.split() if not s.isdigit()])

df['processed_review']=df['lemmatize_text'].apply(lambda review: remove_number(review))

df.head()

voc_size=20000
encoded_docs=[one_hot(d,voc_size) for d in df['processed_review']]

encoded_docs[1]

max_length=30
padded_reviews=pad_sequences(encoded_docs,maxlen=max_length,padding='pre')

padded_reviews[1]

lstm_units=500
model=Sequential()
model.add(Embedding(voc_size,30,input_length=max_length))
model.add(LSTM(lstm_units,dropout=0.2 ,recurrent_dropout=0.2))
model.add(Dense(6,activation='softmax'))
model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()

Y_train=df['reviews.rating']
encoded_data=to_categorical(Y_train)

model.fit(padded_reviews, encoded_data, batch_size=34, epochs=50, verbose=1, validation_split=0.1)

model.history.history.keys()

import matplotlib.pyplot as plt
plt.plot(model.history.history['accuracy'])
plt.plot(model.history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','val'],loc='upper left')
plt.show()

import matplotlib.pyplot as plt
plt.plot(model.history.history['loss'])
plt.plot(model.history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['train','val'],loc='upper left')
plt.show()

model.save('LSTM_Model_Sentiment_Analysis.h5')

encoded_docs[0]

p1=model.predict(encoded_docs[0])

p1

## The output p1 is of two dimension array
## It represents the vector of each word  with respect to the class. For eg: [0,1,2,3,4,5] 
## we can find the  class of review by choosing the max out of the individual word vector and averaging the final 1D array